---
title: "Assignment #1 - chapter 2"
output: pdf_document
authors: Madi Kassymbekov
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
source("HW1-Q1.R")
source("HW1-Q2.R")
```

## **Question 1**
a.Using Caret tuning function, optimal alpha is 0.97 and optimal lambda is 1.
MSE is 975.9 and MAE is 19.64668
R Code
```{r eval=FALSE}
set.seed(123456)
alpha.grid <- seq(0, 1, 0.01)
srchGrd <- expand.grid(.alpha = alpha.grid, .lambda = "all")
elnet.fit <- train(
  Sale_Price ~., data = amesdumtrain, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10, tuneGrid = srchGrd
)

optimal.alpha  <- elnet.fit$bestTune$alpha 
optimal.lambda <- elnet.fit$bestTune$lambda

optimal.elnet.fit <- glmnet(xdumtrain, amesdumtrain$Sale_Price, alpha = optimal.alpha, lambda = optimal.lambda)

predoptimalelnet <- predict(optimal.elnet.fit, newx=xdumtest)

Q1results <- data.frame("Optimal Elastic Net Caret", mean((predoptimalelnet-amesdumtest$Sale_Price)^2), 
                      mean(abs(predoptimalelnet-amesdumtest$Sale_Price)), optimal.alpha, optimal.lambda)
names(Q1results) <- c("Model", "MSE", "MAE", "Alpha", "Lambda")
```

R output Results
```{r, echo=FALSE, warning=FALSE}
Q1results[1,]
```
b.Using own 10-FOld CV function, optimal alpha is 0.05 and lambda is 4. Due to inefficient code and cpu constraints lambda sequence was used between 1 and 4. Best MSE is 984.5199 and best MAE is 19.89811. As caret is well-tested and has optimal code base, it outperformed my own function but end result of own cv function is not way too off from caret optimal estimates. 
```{r eval=FALSE}
cv.optimalElasticNet <- function(seed, x, y, fold, alpha, lambda, xtrain, ytrain, xtest, ytest) {
  
  set.seed(seed)
  
  #Create equally sized folds
  folds <- cut(seq(1,nrow(x)),breaks=fold,labels=FALSE)
  #alpha lambda grid search
  searchgrid <- expand.grid(alpha, lambda)
  grid <- nrow(searchgrid)
  #Store intermediate results for MSE and MAE for alpha-lambda combinations
  tuneResults <- cbind(searchgrid, data.frame(matrix(0, ncol = fold, nrow=grid)))
  #Perform cross validation to tune alpha and lambda based on MSE like caret does
  for (i in 1:grid) {
    for(j in 1:fold){
      #Segment data by fold using the which() function 
      testIndexes <- which(folds==j,arr.ind=TRUE)
      testx <- x[testIndexes, ]
      testy <- y[testIndexes]
      trainx <- x[-testIndexes, ]
      trainy <- y[-testIndexes]
      #Fit and test the alpha-lambda combination for elastic net
      model.fit <- glmnet(trainx , trainy , alpha = tuneResults[i,1] , lambda = tuneResults[i,2])
      model.pred <- predict(model.fit , testx , s=model.fit$lambda)
      meanSquaredError <- mean((model.pred - testy)^2)
      
      tuneResults[i, j+2] <- meanSquaredError
    }
  }
  #Calculate average MSE and use that alpha-lambda for optimal model
  tuneResults$MSE_AVG <- rowMeans(tuneResults[,3:(3+fold-1)])
  optimalAlpha <- tuneResults[which(tuneResults$MSE_AVG==min(tuneResults$MSE_AVG)), 1]
  optimalLambda <- tuneResults[which(tuneResults$MSE_AVG==min(tuneResults$MSE_AVG)), 2]
  
  optimal.model.fit <- glmnet(xtrain , ytrain , alpha = optimalAlpha , lambda = optimalLambda)
  optimal.model.pred <- predict(optimal.model.fit , xtest , s=model.fit$lambda)
  
  optimalMSE <- mean((optimal.model.pred - ytest)^2)
  optimalMAE <- mean(abs(optimal.model.pred - ytest))
  
  optimalResult <- data.frame("Optimal Elastic Net No Caret", optimalMSE, 
                              optimalMAE, optimalAlpha, optimalLambda)
  names(optimalResult) <- c("Model", "MSE", "MAE", "Alpha", "Lambda")
  return(optimalResult)
}

#Run 10fold CV Elastc Net

lambda.grid <- seq(1,4, 0.1)
ElasticNetNoCaret <- cv.optimalElasticNet(123456, xdum, amesdum$Sale_Price, 10, alpha.grid, lambda.grid, xdumtrain, amesdumtrain$Sale_Price, xdumtest, amesdumtest$Sale_Price)
Q1results[nrow(Q1results) + 1,] = list(Model=ElasticNetNoCaret$Model, MSE=ElasticNetNoCaret$MSE, 
                                       MAE=ElasticNetNoCaret$MAE, Alpha=ElasticNetNoCaret$Alpha, Lambda=ElasticNetNoCaret$Lambda)
```
R code output
```{r, echo=FALSE, warning=FALSE}
Q1results[2,]
```
c.Both caret and own optimal elastic net regression models performed worse than models on slide 83 except simple OLS regression, which suggests that there are highly correlated variables in the model which leads to multicollinearity and therefore affect coefficients or the model complexity of the elastic model is too high which leads to overfitting. Multicollinearity problem is dealt well by the Ridge regression model, while Lasso regression performs feature selection which leads to the end model to become less complex and therefore prevent overfitting. In this terms, elastic net as a compromise between Ridge and Lasso seems not a good fit for ames dataset.
![Slide 83 Models]("Q1Models.png"){height=50%}

## **Question 2 **
After fitting various regression models on music dataset, own optimal elastic net cross validation model performed the a little better in terms of MSE (265.5898), while forward selection model performed a little better than others in terms of MAE (12.55716). It should be noted that all models performed similarly in terms of error and there is no obvious best model which outperforms others by a huge margin, however overall, elastic net models performed a little better than lasso, ridge and OLS. 
```{r eval=FALSE}
library(glmnet)
library(caret)
library(MASS)
#Read datasets
trainset <- read.csv("data/music_origin_lat_train_set.csv", header=TRUE) 
testset <- read.csv("data/music_origin_lat_test_set.csv", header=TRUE)
#Subsets of x
xtrain <- trainset[,1:68]
xtest <- testset[,1:68]
musicset <- rbind(trainset, testset)
xmusicset <- musicset[,1:68]
#OLS with all variables as benchmark
lm.fit <- lm(y~., data=trainset)

lm.predict <- predict(lm.fit, newdata=testset)
#Aggregated data frame of different models' performance
Q2results <- data.frame("OLS", mean((lm.predict-testset$y)^2), 
                      mean(abs(lm.predict-testset$y)))
names(Q2results) <- c("Model", "MSE", "MAE")

#Stepwise, forward, backward based on AIC
set.seed(123)
fullmodel <- lm(y~., data=trainset)
emptymodel <- lm(y~1, data=trainset)
backward <- stepAIC(fullmodel,direction="backward", k=2)
forward <- stepAIC(emptymodel,direction="forward",scope=list(upper=fullmodel,lower=emptymodel), k=2)
stepwise <- stepAIC(emptymodel,direction="both",scope=list(upper=fullmodel,lower=emptymodel), k=2)

backward.predict <- predict(backward, newdata = testset)
forward.predict <- predict(forward, newdata = testset)
stepwise.predict <- predict(stepwise, newdata = testset)

Q2results[nrow(Q2results) + 1,] = list(Model="backward", MSE=mean((backward.predict-testset$y)^2), 
                                   MAE=mean(abs(backward.predict-testset$y)))
Q2results[nrow(Q2results) + 1,] = list(Model="forward", MSE=mean((forward.predict-testset$y)^2), 
                                   MAE=mean(abs(forward.predict-testset$y)))
Q2results[nrow(Q2results) + 1,] = list(Model="stepwise", MSE=mean((stepwise.predict-testset$y)^2), 
                                   MAE=mean(abs(stepwise.predict-testset$y)))

#Ridge Regression
ridge.fit <- cv.glmnet(as.matrix(xtrain), trainset$y, alpha = 0)
plot(ridge.fit)

predridge <- predict(ridge.fit, new=as.matrix(xtest), s="lambda.min")

Q2results[nrow(Q2results) + 1,] = list(Model="Ridge Regression", MSE=mean((predridge-testset$y)^2), 
                                   MAE=mean(abs(predridge-testset$y)))

#Lasso Regression
lasso.fit <- cv.glmnet(as.matrix(xtrain), trainset$y, alpha = 1)
plot(lasso.fit)

predlasso <- predict(lasso.fit, new=as.matrix(xtest), s="lambda.min")

Q2results[nrow(Q2results) + 1,] = list(Model="Lasso Regression", MSE=mean((predlasso-testset$y)^2), 
                                   MAE=mean(abs(predlasso-testset$y)))

#Elastic net with alpha=0.5, 0.2 and 0.8
elnet0.2.fit <- cv.glmnet(as.matrix(xtrain), trainset$y, alpha = 0.2)
elnet0.5.fit <- cv.glmnet(as.matrix(xtrain), trainset$y, alpha = 0.5)
elnet0.8.fit <- cv.glmnet(as.matrix(xtrain), trainset$y, alpha = 0.8)

predelnet0.2 <- predict(elnet0.2.fit, new=as.matrix(xtest), s="lambda.min")
predelnet0.5 <- predict(elnet0.5.fit, new=as.matrix(xtest), s="lambda.min")
predelnet0.8 <- predict(elnet0.8.fit, new=as.matrix(xtest), s="lambda.min")


Q2results[nrow(Q2results) + 1,] = list(Model="Elastic Net Alpha=0.2", MSE=mean((predelnet0.2-testset$y)^2), 
                                   MAE=mean(abs(predelnet0.2-testset$y)))
Q2results[nrow(Q2results) + 1,] = list(Model="Elastic Net Alpha=0.5", MSE=mean((predelnet0.5-testset$y)^2), 
                                   MAE=mean(abs(predelnet0.5-testset$y)))
Q2results[nrow(Q2results) + 1,] = list(Model="Elastic Net Alpha=0.8", MSE=mean((predelnet0.8-testset$y)^2), 
                                   MAE=mean(abs(predelnet0.8-testset$y)))

#Elastic net with optimal alpha and lambda using caret
alpha.grid <- seq(0, 1, 0.01)
srchGrd = expand.grid(.alpha = alpha.grid, .lambda = "all")

elnet.fit <- train(
  y ~., data = trainset, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10, tuneGrid = srchGrd
)

optimal.alpha  <- elnet.fit$bestTune$alpha 
optimal.lambda <- elnet.fit$bestTune$lambda

optimal.elnet.fit <- glmnet(as.matrix(xtrain), trainset$y, alpha = optimal.alpha, lambda = optimal.lambda)

predoptimalelnet <- predict(optimal.elnet.fit, newx=as.matrix(xtest))

Q2results[nrow(Q2results) + 1,] = list(Model="Optimal Elastic Net Caret", MSE=mean((predoptimalelnet-testset$y)^2), 
                                   MAE=mean(abs(predoptimalelnet-testset$y)))

#Elastic net with own function 
cv.optimalElasticNet <- function(seed, x, y, fold, alpha, lambda, xtrain, ytrain, xtest, ytest) {
  
  set.seed(seed)
  #Create equally sized folds
  folds <- cut(seq(1,nrow(x)),breaks=fold,labels=FALSE)
  #alpha lambda grid search
  searchgrid <- expand.grid(alpha, lambda)
  grid <- nrow(searchgrid)
  #Store intermediate results for MSE and MAE for alpha-lambda combinations
  tuneResults <- cbind(searchgrid, data.frame(matrix(0, ncol = fold , nrow=grid)))
  #Perform cross validation to tune alpha and lambda based on MSE like caret does
  for (i in 1:grid) {
    for(j in 1:fold){
      #Segment data by fold using the which() function 
      testIndexes <- which(folds==j,arr.ind=TRUE)
      testx <- x[testIndexes, ]
      testy <- y[testIndexes]
      trainx <- x[-testIndexes, ]
      trainy <- y[-testIndexes]
      #Fit and test the alpha-lambda combination for elastic net
      model.fit <- glmnet(as.matrix(trainx) , trainy , alpha = tuneResults[i,1] , lambda = tuneResults[i,2])
      model.pred <- predict(model.fit , as.matrix(testx) , s=model.fit$lambda)
      meanSquaredError <- mean((model.pred - testy)^2)
      
      tuneResults[i, j+2] <- meanSquaredError
    }
  }
  #Calculate average MSE and use that alpha-lambda for optimal model
  tuneResults$MSE_AVG <- rowMeans(tuneResults[,3:(3+fold-1)])
  optimalAlpha <- tuneResults[which(tuneResults$MSE_AVG==min(tuneResults$MSE_AVG)), 1]
  optimalLambda <- tuneResults[which(tuneResults$MSE_AVG==min(tuneResults$MSE_AVG)), 2]
  
  optimal.model.fit <- glmnet(as.matrix(xtrain) , ytrain , alpha = optimalAlpha , lambda = optimalLambda)
  optimal.model.pred <- predict(optimal.model.fit , as.matrix(xtest) , s=model.fit$lambda)
  
  optimalMSE <- mean((optimal.model.pred - ytest)^2)
  optimalMAE <- mean(abs(optimal.model.pred - ytest))
  
  #optimalResult <- list("Optimal Elastic Net No Caret", optimalMSE, optimalMAE)
  optimalResult <- data.frame("Optimal Elastic Net No Caret", optimalMSE, 
                              optimalMAE)
  names(optimalResult) <- c("Model", "MSE", "MAE")
  return(optimalResult)
}

alpha.grid <- seq(0, 1, 0.01)
lambda.grid <- seq(1,4, 0.1)
ElasticNetNoCaret <- cv.optimalElasticNet(123456, xmusicset, musicset$y, 10, alpha.grid, lambda.grid, xtrain, trainset$y, xtest, testset$y)
Q2results[nrow(Q2results) + 1,] = list(Model=ElasticNetNoCaret$Model,MSE=ElasticNetNoCaret$MSE, MAE=ElasticNetNoCaret$MAE)


#Relax Lasso Regression
relax.lasso.fit <- cv.glmnet(as.matrix(xtrain), trainset$y, alpha = 1, relax=TRUE)

predrelaxlasso <- predict(relax.lasso.fit, new=as.matrix(xtest), s="lambda.min", gamma="gamma.min")

Q2results[nrow(Q2results) + 1,] = list(Model="Relaxed Lasso", MSE=mean((predrelaxlasso-testset$y)^2), 
                                   MAE=mean(abs(predrelaxlasso-testset$y)))
```
R code end output with results
```{r, echo=FALSE, warning=FALSE}
Q2results
```



## **Question 3: Multiple Choice Questions**

**1.Which of the following statements is true? (Only one statement is true)**

A.Lasso regression L1-norm penalty's purposes are feature selection, fight with overfitting and smoothing.    

Answer: False. Primary purpose of L1 penalty is to fight with overfitting. In Lasso, as the penalty increases, more features are driven to zero which is used for feature selection for the best model. However, L1 loss function does not have continuous derivatives and therefore smoothing is not one of the primary goals of Lasso regression.    

B. As Ridge regression regularization parameter increases, regression coefficients also increase.   

Answer: False. While regularization parameter increases, coefficients are reduced to near zero, however they are not discarded like in Lasso regression and therefore Ridge regression is not used for feature selection but to deal with multicollinearity.    

C. The Lasso Regression regularization parameter increases the sparsity of the resulting solutions.   

Answer: True. Lasso regression regularization parameter is L1-norm penalty which means as the penalty increases, more parameters will be driven to zero, therefore more sparse resulting solutions.    

D. Multicollinearity affects the prediction capability of the model.    

Answer: False. Multicollinearity affects the coefficient estimates, standard errors and p-values estimates of predictors and intercept which makes it much harder to understand the effect of the predictor on the target variable but it does not affect the prediction capability or  goodness of fit.    

**2.What can be concluded about a model based on a tuning lambda parameter of Ridge Regression?**

A. In case of very large lambda; bias is low, variance is high.  

Answer: False. Very large lambda means the low complexity of the model which by bias-variance tradeoff
signifies a model with high bias and low variance.    

B. In case of very small lambda; bias is low, variance is high.   

Answer: True. Very small lambda means the high comlexity of the model which by bias-variance tradeoff
signifies a model with high variance and small bias.    

C. When lambda is 0, model is uninterpretable.    

Answer: False. When lambda is 0, Ridge Regression will act as an OLS regression and therefore all the predictors can be interpreted.    

D. Lambda value affects the coefficients of predictors which can be used as a guideline for feature selection.        

Answer: False. It's true that large lambda will shrink coefficient parameters towards zero, however it will not discard less important predictors by setting them to zero. Therefore, Ridge regression is not suitable for feature selection, while Lasso regression is used for that.